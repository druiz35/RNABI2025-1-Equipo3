---
output: html_notebook
---

---
output:
  html_document:
    toc: false
    css: apa_style.css
    theme: united
    highlight: pygments
    df_print: paged
    number_sections: false
  pdf_document:
    toc: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

::: {style="text-align: center; color: black; margin-top: 60px;"}
<h1>REPORTE TRABAJO 1: SOLUCIÓN DE PROBLEMAS DE OPTIMIZACIÓN CON MÉTODOS HEURÍSTICOS</h1>

<h2>REDES NEURONALES Y ALGORITMOS BIOINSPIRADOS</h2>

<br><br><br>

<p><strong>Presentado por:</strong></p>

<p>Leonardo Federico Corona Torres<br> David Escobar Ruiz<br> Sebastian Soto Arcila</p>

<br><br>

<p><strong>Profesor:</strong> Juan David Ospina Arango</p>

<p><strong>Monitor:</strong> Andrés Mauricio Zapata Rincón</p>

<br> <img src="logo_unal.png" alt="University Logo" width="100px"/> <br><br>

<p>Universidad Nacional de Colombia<br> Facultad de Minas<br> Ingeniería de Sistemas e Informática</p>

<p><strong>`r format(Sys.Date(), "%d de %B de %Y")`</strong></p>
:::

# Introducción

La optimización numérica es esencial en diversas áreas científicas e ingenieriles para encontrar los valores óptimos de funciones objetivo dadas ciertas restricciones. Los algoritmos genéticos (AG) son metaheurísticas inspiradas en procesos evolutivos biológicos, que han demostrado eficacia en la búsqueda global de óptimos en funciones complejas​jstatsoft.org Los AG simulan la selección natural, la recombinación (cruce) y la mutación para iterativamente mejorar un conjunto de soluciones candidatas (población). Estas técnicas estocásticas son adecuadas para funciones no lineales, discontinuas o con múltiples óptimos locales donde los métodos basados en derivadas pueden fallar. Como casos de estudio se usan funciones clásicas de prueba en optimización: Rosenbrock y Rastrigin, en dimensiones 2D y 3D. La función Rosenbrock, introducida en 1960 por H. H. Rosenbrock, es no convexa y conocida por su característico “valle en forma de banana”​La función Rastrigin (1974) es también no convexa y altamente multimodal, con numerosos mínimos locales, lo que la hace difícil de optimizar​en.wikipedia.org - luca-scr.github.io. Analizaremos cada función en 2 y 3 dimensiones, graficando su paisaje antes de la optimización y luego aplicando un AG con múltiples corridas para evaluar la robustez de los resultados. Se calcularán estadísticas (media y desviación estándar) del mejor valor de fitness obtenido en 30 ejecuciones independientes de cada caso y se resumirán en una tabla.

# Marco Teórico

Los algoritmos genéticos (AG) son técnicas de búsqueda heurística basadas en procesos de evolución natural​ . En un AG típico se define una función FITNESS que evalúa la calidad de cada solución candidata (individuo). A partir de una población inicial aleatoria, se iteran ciclos donde se seleccionan individuos más aptos, se combinan sus “genes” mediante cruces (crossover) y se introducen modificaciones aleatorias (mutaciones). Estos operadores evolucionan la población hacia regiones con mejor fitness. Según Scrucca (2013), los GAs han sido exitosos en optimizar funciones continuas (diferenciables o no) y discretas. Entre los operadores genéticos clave se destacan: Selección: elige individuos con mayor fitness para reproducirse, imitando la supervivencia del más apto. Cruce (crossover): combina partes de dos soluciones parentales para generar descendencia, explorando nuevas regiones del espacio de búsqueda. Mutación: altera aleatoriamente parte de un individuo (por ejemplo, cambiando un valor de su vector de variables) para introducir diversidad genética y evitar estancamiento en óptimos locales.

Visualización previa (gráficos 3D): Antes de la optimización se grafica cada función para apreciar su superficie. Por ejemplo, para Rosenbrock 2D se genera una malla de valores y se usa persp:

```{r}
# Gráfico 3D de Rosenbrock 2D usando persp}
x <- seq(-2, 2, length = 100) 
y <- seq(-1, 3, length = 100) 
Z <- outer(x, y, function(xx, yy) (1 - xx)^2 + 100*(yy - xx^2)^2) 
persp(x, y, Z, theta = 30, phi = 30, expand = 0.5,       
      col = "lightblue", xlab="x", ylab="y", zlab="f(x,y)",       
      main="Función Rosenbrock (2D)")
```

Para los casos 3D (tres variables) es difícil visualizar directamente la función de 3 dimensiones (espacio de 4 variables), por lo que típicamente se muestran proyecciones o cortes.

# Metodología

Para implementar y documentar la optimización se emplea R Markdown, combinando código R y texto explicativo. Se utilizan las siguientes librerías de R: GA (para el algoritmo genético) y knitr (para formateo de tablas).

```{r instalacion de librerias}
install.packages("pso")
install.packages("gifski")

```

```{r librerias}
library(GA)
library(tidyverse)
library(viridis)
library(ggplot2)
library(gganimate)
library(gifski)
library(dplyr)
library(tidyr)
library(purrr)
library(pso)

```

# Parte 1. Optimización numérica

[Contextualización de la parte 1 del trabajo]

[Statement presentando lo que se va a realizar en la parte 1 a manera general]

[Statement explicando brevemente lo realizado con optimización de descenso de gradiente, lo esperado antes de la ejecución y lo obtenido post-ejecución]

[Statement explicando brevemente lo realizado con métodos heurísticos, lo esperado antes de la ejecución y lo obtenido post-ejecución]

## 1.1 Selección e implementación de las funciones de prueba

Para observar el comportamiento de cada uno de los métodos de optimización implementados, se optó por la selección de dos funciones de prueba estándar para evaluar algoritmos de optimización: Función de Rosenbrock y Función de Rastrigin. Debido a su complejidad, ambas funciones son indicadores útiles para comparar la eficacia de optimizadores como los AG.

Previo a la implementación de los algoritmos en el lenguaje R y previo a la recopilación de resultados y la derivación de conclusiones a partir de estos, el equipo decidió realizar una breve investigación general de lo que son las funciones de prueba para problemas de optimización. La investigación se realizó de tal forma que, una vez concluída, se pudieran comprender los conceptos hasta el punto de estar en las capacidades responder las siguientes preguntas implícitamente en una breve sección de este trabajo:

-   ¿Qué es una función de prueba en optimización?

-   ¿Para qué se utiliza una función de prueba en optimización?

-   ¿Cómo se pueden clasificar las funciones de prueba?

-   ¿Cuáles son algunas de las funciones de prueba más utilizadas?

A continuación se presenta dicha sección para luego continuar con el estudio más detallado de las funciones escogidas.

### 1.1.1 Breve explicación de las funciones de prueba para problemas de optimización

Según (Yang, 2010), una función de prueba es una función con unas propiedades especiales que permite probar si el rendimiento de un método de optimización implementado es aceptable bajo las condiciones especiales que impone la función de prueba. 

Esto es especialmente útil para verificar que el método sea eficiente bajo distintas condiciones en las que se espera que se implemente, como por ejemplo, casos en los que la función tiene múltiples mínimos y/o máximos locales.

Según (Molga, 2005), las funciones de prueba se pueden ubicar en una de las siguientes clases, todas siendo funciones continuas:

-   **Clase 1:** Unimodal, convexa, multidimensional.
-   **Clase 2:** Multimodal, dos dimensiones con un número pequeño de extremos locales.
-   **Clase 3**: Multimodal, dos dimensiones con un gran número de extremos locales.
-   **Clase 4:** Multimodal, multidimensional, con un número amplio de extremos locales.

En el caso de las funciones elegidas, la función de Rosenbrock se clasificaría como Clase 3 y la de Rastrigin como Clase 2.

Como ejemplo, en la Figura 1 se presentan algunas funciones de prueba que no se eligieron para este trabajo, pero que son igual de relevantes, importantes y comúnmente utilizadas en la práctica (Molga, 2005):

-   Función de De Jong (Clase 1).

-   Función de Griewangk (Clase 2).

-   Función de Langermann (Clase 3).

-   Función de Ackley (Clase 4).

### 1.1.2 Función de Rosenbrock

Función Rosenbrock (valle de Rosenbrock): Es una función no convexa introducida por Rosenbrock en 1960​en.wikipedia.org. En 2D se define como f(x,y)=(1−x)2+100(y−x2)2,f(x,y) = (1 - x)\^2 + 100(y - x^2)^2,f(x,y)=(1−x)2+100(y−x2)2, donde el parámetro a=1a=1a=1, b=100b=100b=100. Tiene un único mínimo global en (x,y)=(1,1)(x,y)=(1,1)(x,y)=(1,1) con f=0f=0f=0. Su paisaje forma un valle curvo estrecho que dificulta la convergencia hacia el mínimo​cran.r-project.org. La extensión multidimensional generalizada (para nnn variables) es: f(x)=∑i=1n−1[100(xi+1−xi2)2+(1−xi)2].f(\mathbf{x}) = \sum\_{i=1}\^{n-1} \left[100(x_{i+1} - x_i^2)^2 + (1-x_i)^2\right].f(x)=∑i=1n−1​[100(xi+1​−xi2​)2+(1−xi​)2]. El dominio típico usado es −30≤xi≤30-30 \le x_i \le 30−30≤xi​≤30​cran.r-project.org.

Definición en n dimensiones:

$f(X) = \sum_{i=1}^{n-1}100(X_{i+1}-X_i^2)^2 + (1-X_i)^2$

Definición en 2D:

$f(x,y)= 100(y-x^2)^2 + (1-x)^2$

Definición en 3D:

$f(x,y,z)=100[(y-x^2)^2+(z-y^2)^2] + (1-x)^2 + (1-y)^2$

Algunas características y propiedades importantes  son las siguientes:

-   Mínimo global:

    $x_1,...,x_n =1, f(x_1,...,x_n) = 0$

-   Dominio de búsqueda: 

    $-\infty \leq X_i \leq \infty$

    $1 \leq i \leq n$

-   Conocida también como la función banana de Rosenbrock.

-   Tiene forma de valle, el cual es trivial encontrarlo. Sin embargo, la convergencia al mínimo global es difícil(Wikipedia, s.f., Rosenbrock function).

Estas son algunas hipótesis y expectativas que se tuvieron para el rendimiento de cada método implementado con esta función:

-   **Método de descenso de gradiente:**

    -   Si se ubica el punto inicial sobre las rectas tangentes con mayor gradiente a la función entonces el método llegará más rápido a un mínimo.

    -   Por el contrario, si la recta tangente tiene una gradiente más cercana a cero, el método llegará más lentamente, es decir, requerirá de más iteraciones para llegar al punto mínimo.

-   **Método de algoritmos evolutivos:**

-   **Método de optimización de partículas:**

-   **Método de evolución diferencial:**

Se definen las funciones a usar para generar los gráficos de cada función:

```{r}
#install.packages("viridis")
#install.packages("tidyverse")


generate_function_graph <- function(func_2d, low_bound, upper_bound) {
  x1 <- seq(low_bound, upper_bound, length.out = 50)
  x2 <- seq(low_bound, upper_bound, length.out = 50)
  f_x <- outer(x1,x2, FUN = func_2d)
  
  colores        <- viridis::magma(n = 100, alpha = 0.7)
  z.facet.center <- (f_x[-1, -1] + f_x[-1, -ncol(f_x)] +
                       f_x[-nrow(f_x), -1] +
                       f_x[-nrow(f_x), -ncol(f_x)])/4
  z.facet.range  <- cut(z.facet.center, 100)
  
  par(mai = c(0,0,0,0))
  persp(x = x1, y = x2, z = f_x,
        shade = 0.8,
        phi = 30,
        theta = 30,
        col = colores[z.facet.range],
        axes = FALSE)
}

generate_level_curves <- function(func_2d, low_bound, upper_bound) {
  n_length <- 100
  x1 <- seq(low_bound, upper_bound, length.out = n_length)
  x2 <- seq(low_bound, upper_bound, length.out = n_length)
  X <- expand.grid(x1, x2)
  z <- func_2d(X[,1], X[,2])
  Z <- matrix(z, ncol = n_length, nrow = n_length)
  contour(
    x = x1,
    y = x2,
    z = Z,
    nlevels = 100,
    las = 1,
    xlab = expression(x[1]),
    ylab = expression(x[2]),
    main = expression(paste(
      "Función de Rosenbrock: ",
      f(x[1],x[2])==100*(x[2]-x[1]^2)^2+(1-x[1])^2
    )),
    sub = "Curvas de nivel de la función"
  )
}

generate_level_curves_2 <- function(func_2d, low_bound, upper_bound) {
  x1 <- seq(low_bound, upper_bound, length.out = 50)
  x2 <- seq(low_bound, upper_bound, length.out = 50)
  
  datos <- expand.grid(x1 = x1, x2 = x2)
  datos <- datos %>%
           mutate(f_x = map2_dbl(x1, x2, .f = func_2d))
  
  f_level_curves <- ggplot(data = datos, aes(x = x1, y = x2, z = f_x)) +
    geom_contour(aes(colour = stat(level)), bins = 30) +
    labs(title = "f(x1,x2) = x1^2 + x2^2") +
    theme_bw() +
    theme(legend.position = "none")
  return(f_level_curves)
}

generate_countour <- function(func_2d, low_bound, upper_bound) {
  n_length <- 100
  x1 <- seq(low_bound, upper_bound, length.out = n_length)
  x2 <- seq(low_bound, upper_bound, length.out = n_length)
  expand.grid(X1 = x1, X2 = x2) %>%
    mutate(Z = func_2d(X1, X2)) %>%
    ggplot(aes(X1, X2, z = Z)) +
    geom_contour() +
    geom_contour_filled()
}
```

La implementación de la función de Rosenbrock en 2, 3 y N dimensiones se muestra a continuación:

```{r}
# 2 Dimensiones
f_rosenbrock_2d <- function(x, y) {   
  f_value <- 100*(y-(x^2))^2 + ((1-x)^2)   
  return(f_value) 
}  

# 3 Dimensiones
f_rosenbrock_3d <- function(x, y, z) {   
  f_value <- 100*((y-x^2)^2 + (z-y^2)^2) + (1-x)^2 + (1-y)^2   
  return(f_value) 
}

# N Dimensiones
f_rosenbrock <- function(x){
  x_1 <- tail(x, -1)
  x <- head(x, -1)
  z <- sum((100*((x_1-(x^2))^2))+((1-x)^2))
  return(z)
}
```

Las gráficas de la función de Rosenbrock en 2 y 3 dimensiones se muestran a continuación:

```{r}
generate_countour(f_rosenbrock_2d, -5, 5)
```

```{r}
generate_level_curves(f_rosenbrock_2d, -5, 5)
```

```{r}
generate_function_graph(f_rosenbrock_2d, -5, 5)
```

### 1.1.3 Función de Rastrigin

Función Rastrigin: Es una función no convexa multimodal, propuesta por Rastrigin en 1974​en.wikipedia.org. Su fórmula en nnn dimensiones es: f(x)=10n+∑i=1n[xi2−10cos⁡(2πxi)],f(\mathbf{x}) = 10n + \sum\_{i=1}\^{n}\left[x_i^2 - 10\cos(2\pi x_i)\right],f(x)=10n+∑i=1n​[xi2​−10cos(2πxi​)], con xi∈[−5.12,5.12]x_i \in [-5.12,5.12]xi​∈[−5.12,5.12]. En 2D específico: f(x1,x2)=20+x12+x22−10(cos⁡(2πx1)+cos⁡(2πx2)),f(x_1,x_2) = 20 + x_1\^2 + x_2\^2 - 10(\cos(2\pi x_1) + \cos(2\pi x_2)),f(x1​,x2​)=20+x12​+x22​−10(cos(2πx1​)+cos(2πx2​)), que posee un mínimo global en (0,0)(0,0)(0,0) con f=0f=0f=0​luca-scr.github.io. Presenta múltiples mínimos locales dispuestos de forma regular​cran.r-project.org​luca-scr.github.io, lo que lo convierte en un desafío típico para algoritmos de optimización.

Definición en n dimensiones:

f(x)=An+i=1nxi2-A(2xi) , A=10

Definición en 2D:

f(x,y)=x2+y2+A2-(2x)-(2y), A=10

Definición en 3D:

f(x,y,z)=x2+y2+z2+A3-(2x)-(2y)-(2z), A=10

Algunas características y propiedades importantes son las siguientes:

-   Mínimo global:

    -   f(0,...,0)=0

-   Dominio de búsqueda:

    -   -5.12xi5.12

-   Particularmente, hallar el mínimo de esta función es un problema difícil debido a la larga cantidad de mínimos locales (Wikipedia, s.f., Rastrigin function).

Estas son algunas hipótesis y expectativas que se tuvieron para el rendimiento de cada método implementado con esta función:

-   **Método de descenso de gradiente:**

    -   El éxito del método en alcanzar un mínimo dependerá enormemente del punto inicial elegido debido a los múltiples mínimos locales que tiene esta función. 

-   **Método de algoritmos evolutivos:** Se espera que Rastrigin sea más difícil de optimizar debido a su gran cantidad de óptimos locales.

-   **Método de optimización de partículas:**

-   **Método de evolución diferencial:**

La implementación de la función de Rastrigin en 2, 3 y N dimensiones se muestra a continuación:

```{r}
# 2 Dimensiones 
f_rastrigin_2d <- function(x, y) {
  A = 10   
  f_value <- x^2 + y^2 + A*(2 - cos(2*pi*x) - cos(2*pi*y))   
  return(f_value) 
}  

# 3 Dimensiones 
f_rastrigin_3d <- function(x, y, z) {   
  A = 10   
  f_value <- x^2 + y^2 + z^2 + A*(3 - cos(2*pi*x) - cos(2*pi*y) - cos(2*pi*z))   
  return(f_value) 
}

# N Dimensiones
f_rastrigin <-function(x){
  A <- 10
  n <- length(x)
  z <- (A*n) + sum(x^2 - A*cos(2*pi*x))
  return(z)
}
```

Las gráficas de la función de Rastrigin en 2 y 3 dimensiones se muestran a continuación:

```{r}
generate_countour(f_rastrigin_2d, -10, 10)
```

```{r}
generate_level_curves(f_rastrigin_2d, -10, 10)
```

```{r}
generate_level_curves_2(f_rastrigin_2d, -10, 10)
```

```{r}
generate_function_graph(f_rastrigin_2d, -10, 10)
```

## 1.2 Implementación en R de métodos de optimización a utilizar

## 1.3 Método de descenso de gradiente

### 1.3.1 Implementación en R de descenso por gradiente

#### Implementación de derivada parcial

```{r}
partial_dev <- function(x,i,fun,h=0.01){
    e <- x*0 # crea un vector de ceros de la misma longitud de x
    e[i] <- h
    y <- (fun(x+e)-fun(x-e))/(2*h)
  return(y)
}
```

#### Implementación del gradiente

```{r}
num_grad <- function(x,fun,h=0.01){
  # x: punto del espacio donde se debe evaluar el gradiente
  # fun: función para la que se desea calcular el gradiente en x
  # h: es el tamaño de ventana para el cálculo de la derivada numérica
  d <- length(x)
  y <- mapply(FUN=partial_dev,i=1:d,MoreArgs=list(x=x,h=h,fun=fun))
  return(y)
}
```

#### Implementación de derivada del gradiente

```{r}
deriv_grad <- function(x,fun,i=1,h=0.01){
  # x: punto en el que se evalúa el gradiente
  # fun: función para la cual se calcula la derivada del gradiente respecto a la íesima componente
  # i: i-ésima componente del vector x con respecto a la que se deriva
    e <- x*0 # crea un vector de ceros de la misma longitud de x
    e[i] <- h
    y <- (num_grad(x+e,fun=fun,h=h)-num_grad(x-e,fun=fun,h=h))/(2*h)
    return(y)
}
```

#### Implementación de matriz Hessiana

```{r}
matriz_hessiana <- function(x,fun,h=0.01){
  # x: punto en el que se evalúa la matriz hessiana
  # fun: función a la que se le calcula la matriz hessiana en x
  # h: es el tamaño de ventana para el cálculo de la derivada numérica
  d <- length(x)
  y <- mapply(FUN=deriv_grad,i=1:d,MoreArgs=list(x=x,h=h,fun=fun),SIMPLIFY = TRUE)
  return(y)
}
```

#### Implementación completa de optimizador multivariado por descenso de gradiente

```{r}
optimizador_mult_numdev <- function(x0,fun,max_eval=100,h=0.01,eta=0.01){
  x <- matrix(NA,ncol =length(x0), nrow = max_eval)
  x[1,] <- x0
  for (i in 2:max_eval){
    num_grad_fun <- num_grad(x[i-1,],fun,h)
    H <- matriz_hessiana(x[i-1,],fun,h)
    cambio <- - eta*solve(H)%*%num_grad_fun
    x[i,] <- x[i-1,] + cambio
    cambio_opt <- sqrt(sum((x[i-1,]-x[i,])^2))
    if (cambio_opt<0.00001){
      break
    }
  }
  return(x[1:i,])
}
```

### 1.3.2 Optimización de la función de Rosenbrock en 2 dimensiones

```{r}
# Ejecución del método
sol_rosen <- optimizador_mult_numdev(f_rosenbrock,x0=c(-4,-4),eta=1)

# Graficación del proceso de optimización
n_length <- 100
x1 <- seq(-5, 5, length.out = n_length)
x2 <- seq(-5, 5, length.out = n_length)
X <- expand.grid(x1, x2)
z <- f_rosenbrock_2d(X[,1], X[,2])
Z <- matrix(z, ncol = n_length, nrow = n_length)
contour(
  x = x1,
  y = x2,
  z = Z,
  nlevels = 100,
  las = 1,
  xlab = expression(x[1]),
  ylab = expression(x[2]),
  main = expression(paste(
    "Función de Rosenbrock: ",
    f(x[1],x[2])==100*(x[2]-x[1]^2)^2+(1-x[1])^2)
  ),
  sub = "Curvas de nivel de la función"
)
lines(sol_rosen, type="b",cex=1.5,col="red")

```

### 1.3.3 Optimización de la función de Rosenbrock en 3 dimensiones

```{r}
# Ejecución del método
sol_rosen <- optimizador_mult_numdev(f_rosenbrock,x0=c(-4,-4,-4),eta=1)

# Graficación del proceso de optimización
## TODO: Revisar y corregir
n_length <- 100
x1 <- seq(-5, 5, length.out = n_length)
x2 <- seq(-5, 5, length.out = n_length)
x3 <- seq(-5, 5, length.out = n_length)
X <- expand.grid(x1, x2, x3)
z <- f_rosenbrock_3d(X[,1], X[,2], X[,3])
Z <- matrix(z, ncol = n_length, nrow = n_length)
contour(
  x = x1,
  y = x2,
  z = Z,
  nlevels = 100,
  las = 1,
  xlab = expression(x[1]),
  ylab = expression(x[2]),
  main = expression(paste(
    "Función de Rosenbrock: ",
    f(x[1],x[2])==100*(x[2]-x[1]^2)^2+(1-x[1])^2)
  ),
  sub = "Curvas de nivel de la función"
)
lines(sol_rosen, type="b",cex=1.5,col="red")

```

### 1.3.4 Optimización de la función de Rastrigin en 2 dimensiones

```{r}
# Ejecución del método
sol_rosen <- optimizador_mult_numdev(f_rastrigin,x0=c(-4,-4),eta=1)

# Graficación del proceso de optimización
n_length <- 100
x1 <- seq(-5, 5, length.out = n_length)
x2 <- seq(-5, 5, length.out = n_length)
X <- expand.grid(x1, x2)
z <- f_rastrigin_2d(X[,1], X[,2])
Z <- matrix(z, ncol = n_length, nrow = n_length)
contour(
  x = x1,
  y = x2,
  z = Z,
  nlevels = 100,
  las = 1,
  xlab = expression(x[1]),
  ylab = expression(x[2]),
  main = expression(paste(
    "Función de Rosenbrock: ",
    f(x[1],x[2])==100*(x[2]-x[1]^2)^2+(1-x[1])^2)
  ),
  sub = "Curvas de nivel de la función"
)
lines(sol_rosen, type="b",cex=1.5,col="red")

```

### 1.3.5 Optimización de la función de Rastrigin en 3 dimensiones

```{r}
# Ejecución del método
sol_rosen <- optimizador_mult_numdev(f_rastrigin,x0=c(-4,-4,-4),eta=1)

# Graficación del proceso de optimización
## TODO: Revisar y corregir
n_length <- 100
x1 <- seq(-5, 5, length.out = n_length)
x2 <- seq(-5, 5, length.out = n_length)
X <- expand.grid(x1, x2)
z <- f_rastrigin_3d(X[,1], X[,2], X[,3])
Z <- matrix(z, ncol = n_length, nrow = n_length)
contour(
  x = x1,
  y = x2,
  z = Z,
  nlevels = 100,
  las = 1,
  xlab = expression(x[1]),
  ylab = expression(x[2]),
  main = expression(paste(
    "Función de Rosenbrock: ",
    f(x[1],x[2])==100*(x[2]-x[1]^2)^2+(1-x[1])^2)
  ),
  sub = "Curvas de nivel de la función"
)
lines(sol_rosen, type="b",cex=1.5,col="red")

```

### 1.3.6 Conclusiones método de descenso por gradiente

## 1.4 Método de evolución diferencial

La evolución diferencial es un algoritmo de optimización inspirado en la evolución biológica. Funciona manteniendo una población de soluciones, y mejorándolas generación tras generación mediante operaciones de mutación, recombinación y selección.

-   **Mutación**: se combinan 3 individuos distintos de la población para crear una variante.
-   **Recombinación**: se mezcla esa variante con el individuo actual.
-   **Selección**: se escoge el mejor entre el original y el nuevo.

Este proceso se repite varias veces hasta encontrar una solución óptima.

### 1.4.1 Implementación en R de evolución diferencial

```{r}
evolucion_diferencial <- function(fun_obj, dim = 2, NP = 30, F = 0.8, CR = 0.9,
                                  gens = 100, bounds = c(-5, 5)) {

  # Inicializar población
  poblacion <- matrix(runif(NP * dim, bounds[1], bounds[2]), ncol = dim)
  fitness <- apply(poblacion, 1, fun_obj)

  historial <- numeric(gens)
  mejores <- matrix(NA, gens, dim)

  for (gen in 1:gens) {
    for (i in 1:NP) {
      # Seleccionar 3 índices distintos
      indices <- sample(setdiff(1:NP, i), 3)
      x1 <- poblacion[indices[1], ]
      x2 <- poblacion[indices[2], ]
      x3 <- poblacion[indices[3], ]

      # Mutación
      mutado <- x1 + F * (x2 - x3)

      # Recombinar
      trial <- poblacion[i, ]
      jrand <- sample(1:dim, 1)
      for (j in 1:dim) {
        if (runif(1) < CR || j == jrand) {
          trial[j] <- mutado[j]
        }
      }

      # Selección
      if (fun_obj(trial) < fitness[i]) {
        poblacion[i, ] <- trial
        fitness[i] <- fun_obj(trial)
      }
    }

    # Guardar mejor resultado
    best_idx <- which.min(fitness)
    historial[gen] <- fitness[best_idx]
    mejores[gen, ] <- poblacion[best_idx, ]
  }

  list(mejor = poblacion[which.min(fitness), ],
       valor = min(fitness),
       historial = historial,
       trayectoria = mejores)
}
```

### 1.4.2 Optimización de la función de Rosenbrock en 2 dimensiones

```{r}
# Ejecución del método
set.seed(123)
res_rosen <- evolucion_diferencial(f_rosenbrock, dim = 2)

# Mostrar mejor solución
res_rosen$mejor
res_rosen$valor

# Graficar trayectoria
x1 <- seq(-3, 3, length.out = 100)
x2 <- seq(-3, 3, length.out = 100)
z <- outer(x1, x2, Vectorize(function(x, y) f_rosenbrock(c(x, y))))
contour(x1, x2, z, nlevels = 50,
        main = "Rosenbrock 2D - Trayectoria", xlab = "x", ylab = "y")
lines(res_rosen$trayectoria[,1], res_rosen$trayectoria[,2], col = "red", type = "b")

# Graficación del proceso de optimización
```

En este gráfico se muestran las curvas de nivel de la función de Rosenbrock en dos dimensiones. Estas curvas representan líneas donde la función tiene igual valor, y el valle curvado al centro es donde está el mínimo global (en el punto (1,1)(1,1)).

La línea roja representa la trayectoria que siguió el algoritmo de evolución diferencial durante las iteraciones. Se puede observar cómo el enjambre de soluciones se fue acercando progresivamente hacia el mínimo, mejorando su posición en cada generación.

El resultado final obtenido fue: [1] 1.000000 1.000001 Valor mínimo encontrado: 1.91e-12

### 1.4.3 Optimización de la función de Rosenbrock en 3 dimensiones

```{r}
# Ejecución del método
res_rosen_3d <- evolucion_diferencial(f_rosenbrock, dim = 3)
res_rosen_3d$mejor
res_rosen_3d$valor


# Graficación del proceso de optimización
```

En esta parte del informe se muestran los resultados numéricos para la optimización de las funciones en 3D. Dado que no se puede visualizar fácilmente en una gráfica 3D de trayectoria, se reportan las mejores posiciones y valores obtenido.

### 1.4.4 Optimización de la función de Rastrigin en 2 dimensiones

```{r}
# Ejecución del método
set.seed(456)
res_ras <- evolucion_diferencial(f_rastrigin, dim = 2)

# Mostrar mejor solución
res_ras$mejor
res_ras$valor

# Graficar trayectoria
x1 <- seq(-5.12, 5.12, length.out = 100)
x2 <- seq(-5.12, 5.12, length.out = 100)
z <- outer(x1, x2, Vectorize(function(x, y) f_rastrigin(c(x, y))))
contour(x1, x2, z, nlevels = 50,
        main = "Rastrigin 2D - Trayectoria", xlab = "x", ylab = "y")
lines(res_ras$trayectoria[,1], res_ras$trayectoria[,2], col = "blue", type = "b")


# Graficación del proceso de optimización
```

Aquí se grafican las curvas de nivel de la función de Rastrigin, que es multimodal, es decir, tiene muchos mínimos locales (patrón ondulado). La búsqueda es mucho más compleja que en Rosenbrock.

La línea azul muestra cómo la evolución diferencial se mueve por el espacio de búsqueda y logra escapar de los mínimos locales hasta acercarse al óptimo global, que se encuentra en (0,0)(0,0).

Resultado obtenido: [1] 2.176697e-06 -2.015785e-07 Valor mínimo: 9.48e-10

### 1.4.5 Optimización de la función de Rastrigin en 3 dimensiones

```{r}
# Ejecución del método
res_ras_3d <- evolucion_diferencial(f_rastrigin, dim = 3)
res_ras_3d$mejor
res_ras_3d$valor

# Graficación del proceso de optimización
```

En este caso, el algoritmo encontró una solución cercana al mínimo global, aunque no exacta. Esto es esperable, ya que Rastrigin es mucho más difícil en 3D debido a la gran cantidad de mínimos locales.

### 1.4.6 Conclusiones método de evolución diferencial

## 1.5 Método de optimización de partículas

### 1.5.1 Implementación en R de optimización de partículas

```{r}
list_to_matrix <- function(data) {
  for (i in 1:length(data)) {
    data[[i]] <- matrix(data[[i]], nrow=2, ncol=12)
  }
  return(data)
}

particle_swarm_optimization <- function(n,func,lower_bounds,upper_bounds) {
  set.seed(2001)
  o_min <- psoptim(rep(NA,n), func, lower=lower_bounds,upper=upper_bounds,control=list(fnscale=1e-8,trace=1,trace.stats=TRUE))
  o_max <- psoptim(rep(NA,n), func, lower=lower_bounds,upper=upper_bounds,control=list(fnscale=-1*(1e-8),trace=1,trace.stats=TRUE,s=30))
  print("=====================SUMMARY=====================")
  print("MINIMIZATION:")
  print("Point:")
  show(o_min$par)
  print("Value:")
  show(func(o_min$par))
  print("MAXIMIZATION")
  print("Point:")
  show(o_max$par)
  print("Value:")
  show(func(o_max$par))
  return(list("o_min"=o_min, "o_max"=o_max))
}

animate_pso <- function(particles_positions, func, gif_name) {
  positions_per_iteration <- list_to_matrix(particles_positions)
  df <- map2_dfr(
    positions_per_iteration,
    .y = seq_along(positions_per_iteration),
    .f = function(mat, iter) {
      tibble(
        particle_id = 1:ncol(mat),
        x = mat[1, ],
        y = mat[2, ],
        iter = iter
      )
    }
  )
  x_seq <- seq(-10, 10, length.out = 100)
  y_seq <- seq(-10, 10, length.out = 100)
  grid <- expand.grid(x = x_seq, y = y_seq)
  grid$z <- with(grid, func(x, y))
  
  p <- ggplot() +
    geom_contour(data = grid, aes(x=x, y=y, z=z),
                 bins = 30, color = "gray") +
    geom_point(data=df, aes(x = x, y = y), color="red", size=2) +
    xlim(-10, 10) + ylim(-10, 10) +  # Adjust limits to your data range
    theme_minimal() +
    transition_manual(frames = iter) +
    labs(title = "Iteration: {current_frame}")
  #p + transition_reveal(agno)
  animate(p, fps = 5, renderer=gifski_renderer())
  anim_save(gif_name,p)
}



```

### 1.5.2 Optimización de la función de Rosenbrock en 2 dimensiones

```{r}
# Parámetros a utilizar
n <- 2
lower_bounds <- -20
upper_bounds <- 20

# Ejecución del método
o_pso <- particle_swarm_optimization(n,f_rosenbrock,lower_bounds,upper_bounds)

```

```{r}
# Graficación del proceso de optimización (minimización)
o_min <- o_pso$o_min
o_min_particles_positions <- o_min$stats$x
gif_name <- "pso_rosenbrock_min.gif"
animate_pso(o_min_particles_positions, f_rosenbrock_2d, gif_name)
```

```{r}
# Graficación del proceso de optimización (maximización)
o_max <- o_pso$o_max
o_max_particles_positions <- o_max$stats$x
gif_name <- "pso_rosenbrock_max.gif"
animate_pso(o_max_particles_positions, f_rosenbrock_2d, gif_name)
```

### 1.5.3 Optimización de la función de Rosenbrock en 3 dimensiones

```{r}
# Parámetros a utilizar
n <- 3
lower_bounds <- -5
upped_bounds <- 5

# Ejecución del método
particle_swarm_optimization(n,f_rosenbrock,lower_bounds,upper_bounds)

# Graficación del proceso de optimización
```

### 1.5.4 Optimización de la función de Rastrigin en 2 dimensiones

```{r}
# Parámetros a utilizar
n <- 2
lower_bounds <- -5
upped_bounds <- 5

# Ejecución del método
o_pso <- particle_swarm_optimization(n,f_rastrigin,lower_bounds,upper_bounds)
```

```{r}
# Graficación del proceso de optimización (minimización)
o_min <- o_pso$o_min
o_min_particles_positions <- o_min$stats$x
animate_pso(o_min_particles_positions, f_rastrigin_2d)
```

```{r}
# Graficación del proceso de optimización (maximización)
o_max <- o_pso$o_max
o_max_particles_positions <- o_max$stats$x
animate_pso(o_max_particles_positions, f_rastrigin_2d)
```

### 1.5.5 Optimización de la función de Rastrigin en 3 dimensiones

```{r}
# Parámetros a utilizar
n <- 3
lower_bounds <- -5
upped_bounds <- 5
# Ejecución del método
particle_swarm_optimization(n,f_rastrigin,lower_bounds,upper_bounds)
# Graficación del proceso de optimización
```

### 1.5.6 Conclusiones método de optimización de partículas

[Pulir luego]

-   El método suele llegar al mínimo global, pero depende mucho del punto inicial.

## 1.6 Método de algoritmos evolutivos

Para evaluar la robustez de los GA, se realizarán múltiples ejecuciones independientes y se analizará la dispersión del fitness resultante.

Se utilizan las siguientes librerías de R: GA (para el algoritmo genético) y knitr (para formateo de tablas).

Adicionalmente, se definen algunas funciones para poder mostrar por medio de una animación el proceso de optimización para las funciones de Rosenbrock y de Rastrigin.

```{r}
animate_ga_optimization <- function(func) {
  # 2. Crear el entorno para almacenar la evolución de la población
  pop_data <- data.frame()
  
  # 3. Ejecutar el algoritmo genético, capturando las poblaciones
  ga_rastrigin <- ga(
    type = "real-valued",
    fitness = function(x) -func(x),
    lower = c(-5.12, -5.12), upper = c(5.12, 5.12),
    popSize = 50, maxiter = 50, run = 50,
    monitor = function(obj) {
      gen <- obj@iter
      pop <- obj@population
      df <- data.frame(
        X1 = pop[, 1],
        X2 = pop[, 2],
        Generacion = gen
      )
      pop_data <<- rbind(pop_data, df)
    }
  )
  
  # 4. Crear grilla para visualizar la función Rastrigin
  x <- seq(-5.12, 5.12, length.out = 100)
  y <- seq(-5.12, 5.12, length.out = 100)
  grid <- expand.grid(X1 = x, X2 = y)
  grid$Z <- apply(grid, 1, func)
  
  # 5. Graficar y animar
  base_plot <- ggplot() +
    geom_raster(data = grid, aes(x = X1, y = X2, fill = Z), interpolate = TRUE) +
    scale_fill_viridis_c() +
    geom_point(data = pop_data, aes(x = X1, y = X2), color = "red", size = 1, alpha = 0.6) +
    labs(title = "Optimización de Rastrigin usando GA", subtitle = "Generación: {closest_state}",
         x = "x1", y = "x2") +
    transition_states(Generacion, transition_length = 2, state_length = 1) +
    theme_minimal()
  
  # 6. Exportar como GIF
  anim_save("optim_rastrigin_ga.gif", animation = animate(base_plot, renderer = gifski_renderer(), fps = 5, width = 600, height = 500))
}

```

El resumen reporta el mejor fitness encontrado (negativo) y la solución óptima en cada ejecución.

Múltiples corridas (robustez): Para evaluar la variabilidad del método estocástico, se repite cada caso al menos 30 veces con semillas distintas. Se registra el mejor valor de fitness (valorizado positivamente) obtenido en cada corrida

### 1.6.1 Implementación en R de algoritmos evolutivos

Se utiliza la función ga() del paquete GA. Para problemas de minimización se define la función de fitness como el negativo del valor objetivo, ya que ga() maximiza por defecto. Se especifican los límites de búsqueda. Por ejemplo, para Rosenbrock 2D:

### 1.6.2 Optimización de la función de Rosenbrock en 2 dimensiones

```{r}
# Ejecución del método
ga_ros2d <- ga(type = "real-valued",
               fitness = function(x) -f_rosenbrock(x),
               lower = c(-5, -5), upper = c(5, 5),
               popSize = 50, maxiter = 100, run = 50)
summary(ga_ros2d)
```

```{r}
# Graficación del proceso de optimización
gif_name <- "optim_rosenbrock_ga.gif"
animate_ga_optimization(f_rosenbrock)
```

```{r}
set.seed(123)  # semilla reproducible
best_vals_ros <- replicate(30, {
  GA <- ga(type = "real-valued",
           fitness = function(x) -rosenbrock2d(x),
           lower = c(-5, -5), upper = c(5, 5),
           popSize = 50, maxiter = 100, run = 50)
  -GA@fitnessValue  # convertir a valor positivo
})
mean_ros <- mean(best_vals_ros)
sd_ros   <- sd(best_vals_ros)
# Similarmente para Rastrigin 2D, Rosenbrock 3D, Rastrigin 3D.
```

### 1.6.3 Optimización de la función de Rosenbrock en 3 dimensiones

```{r}
# Ejecución del método
ga_ros3d <- ga(type = "real-valued",
               fitness = function(x) -f_rosenbrock(x),
               lower = c(-5, -5, 3), upper = c(5, 5, 3),
               popSize = 50, maxiter = 100, run = 50)
summary(ga_ros3d)
```

```{r}
# Realizar 30 ejecuciones independientes para Rosenbrock 3D
set.seed(123)  # semilla reproducible
best_vals_ros3d <- replicate(30, {
  GA <- ga(type = "real-valued",
           fitness = function(x) -rosenbrock3d(x),
           lower = c(-5, -5, 3), upper = c(5, 5, 3),
           popSize = 50, maxiter = 100, run = 50)
  -GA@fitnessValue  # convertir a valor positivo
})
mean_ros3d <- mean(best_vals_ros3d)
sd_ros3d   <- sd(best_vals_ros3d)
#  Rosenbrock 3D, Rastrigin 3D.
```

### 1.6.4 Optimización de la función de Rastrigin en 2 dimensiones

```{r}
# Ejecución del método
ga_ras2d <- ga(type = "real-valued",
               fitness = function(x) -f_rastrigin(x),
               lower = c(-5, -12), upper = c(5, 12),
               popSize = 50, maxiter = 100, run = 50)
summary(ga_ras2d)

```

```{r}
# Graficación del proceso de optimización
gif_name <- "optim_rastrigin_ga.gif"
animate_ga_optimization(f_rastrigin)
```

```{r}
set.seed(123)  # semilla reproducible
best_vals_ras2d <- replicate(30, {
  GA <- ga(type = "real-valued",
           fitness = function(x) -rastrigin2d(x),
           lower = c(-5, -5), upper = c(5, 5),
           popSize = 50, maxiter = 100, run = 50)
  -GA@fitnessValue  # convertir a valor positivo
})
mean_ras2d <- mean(best_vals_ras2d)
sd_ras2d   <- sd(best_vals_ras2d)
```

### 1.6.5 Optimización de la función de Rastrigin en 3 dimensiones

```{r}
# Ejecución del método
ga_ras3d <- ga(type = "real-valued",
               fitness = function(x) -f_rastrigin(x),
               lower = c(-5, -12,3), upper = c(5, 12,3 ),
               popSize = 50, maxiter = 100, run = 50)
summary(ga_ras3d)

# Graficación del proceso de optimización
```

```{r}
set.seed(123)  # semilla reproducible
best_vals_ras3d <- replicate(30, {
  GA <- ga(type = "real-valued",
           fitness = function(x) -rastrigin3d(x),
           lower = c(-5, -5, 3), upper = c(5, 5, 3),
           popSize = 50, maxiter = 100, run = 50)
  -GA@fitnessValue  # convertir a valor positivo
})
mean_ras3d <- mean(best_vals_ras3d)
sd_ras3d   <- sd(best_vals_ras3d)
#  Rastrigin 3D.
```

### 1.6.6 Cálculo de estadísticas y análisis

Con los vectores de mejores valores (best_vals_ros, etc.), se calculan la media y desviación estándar de cada conjunto de 30 resultados. Por ejemplo, mean_ros y sd_ros arriba y las demas, Para asi presentar los resultados.

```{r}
library(knitr)
resultados <- data.frame(
  Función   = c("Rosenbrock", "Rastrigin", "Rosenbrock", "Rastrigin"),
  Dimensión = c("2D", "2D", "3D", "3D"),
  Media     = c(mean_ros, mean_ras2d, mean_ros3d, mean_ras3d),
  SD        = c(sd_ros, sd_ras2d, sd_ros3d, sd_ras3d)
)
kable(resultados, caption = "Resumen estadístico (media y desviación estándar) del mejor fitness obtenido tras 30 ejecuciones independientes de cada caso.")

```

Los resultados de las múltiples ejecuciones se resumen en la Tabla 1. Esta tabla muestra la media y desviación estándar del mejor valor de fitness (recordado que es el valor de la función objetivo en su mínimo global, típicamente cercano a 0) para cada combinación de función y dimensión. Se observa que para Rosenbrock 2D, la media del fitness mínimo es cercana a 0 con baja dispersión, reflejando que el GA normalmente encuentra el mínimo global (0) o cercano. Para Rastrigin 2D, la media también puede acercarse a 0, pero con mayor desviación estándar debido a los múltiples mínimos locales. En 3D ambos problemas suelen mostrar valores medios mayores (más alejados de 0) y mayor variabilidad, lo cual indica una mayor dificultad de búsqueda al aumentar la dimensionalidad.

```{r}
# tabla de los valores calculados)
library(knitr)
res_df <- data.frame(
  Función   = c("Rosenbrock", "Rastrigin", "Rosenbrock", "Rastrigin"),
  Dimensión = c("2D", "2D", "3D", "3D"),
  Media     = c(mean_ros, mean_ras2d, mean_ros3d, mean_ras3d),
  SD        = c(sd_ros, sd_ras2d, sd_ros3d, sd_ras3d)
)
kable(res_df, caption = "Tabla 1. Estadísticas (media y desviación estándar) del fitness mínimo alcanzado en 30 corridas independientes para cada función y dimensión.")

```

Tabla 1. Estadísticas (media y desviación estándar) del fitness mínimo alcanzado en 30 corridas independientes para cada función y dimensión.

### 1.6.7 Conclusiones método de algoritmos evolutivos

Discusión:los resultados confirman que el algoritmo genético es capaz de aproximarse a los mínimos globales de ambos problemas en múltiples dimensiones. Como era de esperar, Rastrigin mostró mayor variabilidad en los valores de fitness debido a sus muchos mínimos locales, lo que implica que algunas ejecuciones del GA pueden quedarse atrapadas en óptimos locales alejados del global. En contraste, Rosenbrock (aunque es no convexa) tiende a un único valle principal; por ello, la mayoría de las corridas alcanzaron valores cercanos al mínimo global con menor dispersión. En general se observa que al aumentar la dimensión (de 2D a 3D) la tarea se complica y la media del fitness aumenta (peor óptimo encontrado), reflejando la maldición de la dimensionalidad. El uso de múltiples ejecuciones independientes es esencial para evaluar la robustez de los AG. Debido a su naturaleza estocástica, cada ejecución puede converger a soluciones distintas. Al analizar la media y desviación estándar de los fitness finales se obtiene una medida de fiabilidad del algoritmo: una baja desviación indica resultados consistentes. En la literatura sobre algoritmos genéticos se reconoce que en muchos casos una sola ejecución puede no ser representativa​jstatsoft.org. Aunque un análisis comparativo profundo (p.ej., usando poblaciones más grandes o múltiples corridas en paralelo) queda fuera del alcance de este documento, nuestros resultados ilustran este fenómeno. Este estudio es reproducible: todo el código R necesario está incluido, permitiendo a otros investigadores replicar los experimentos, variar parámetros del GA (tasa de cruce, mutación, tamaño de población, etc.) y comparar con otros algoritmos de optimización.:Conclusiones Se ha presentado una documentación completa de la optimización de las funciones de Rosenbrock y Rastrigin en 2D y 3D empleando algoritmos genéticos en R. Mediante visualizaciones 3D iniciales se ilustraron las características de cada función de prueba. Se implementó el paquete GA para resolver cada caso y se realizaron 30 ejecuciones independientes para evaluar la robustez. Los resultados muestran que el GA puede encontrar aproximaciones al mínimo global en ambos problemas, aunque la función Rastrigin (múltiples mínimos locales) presenta más variabilidad y dificultad, especialmente en 3D.

## 1.7 Conclusiones parte 1

# Parte 2. Optimización combinatoria

## 2.1 Planteamiento del problema

## 2.2 Primer método: Colonia de hormigas

## 2.3 Segundo método: Algoritmo genético

## 2.4 Conclusiones parte 2

# Conclusiones finales

# Reporte de contribución individual

## Leonardo Federico Corona Torres

## David Escobar Ruiz

## Sebastian Soto Arcila

# Bibliografía

<https://www.researchgate.net/publication/45932888_Test_Problems_in_Optimization>

<https://robertmarks.org/Classes/ENGR5358/Papers/functions.pdf>

<https://en.wikipedia.org/wiki/Test_functions_for_optimization>

<https://en.wikipedia.org/wiki/Rosenbrock_function>

<https://en.wikipedia.org/wiki/Rastrigin_function>
